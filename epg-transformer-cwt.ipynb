{"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset, SubsetRandomSampler\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport os\nimport cv2\nimport time\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"tUFRqu4IEt3Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet \\\n--save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \\\n'https://docs.google.com/uc?export=download&id=1vwucTNj66q93cRYjkzKVNxHd5k3Xpm5f' -O- | sed -rn 's/.*confirm=([0-9A-\\\n!Za-z_]+).*/\\1\\n/p')&id=1OfvUx1jCva-TR97_GsguAQpd_0rP4HOx\" -O train_100Hz.zip && rm -rf /tmp/cookies.txt\n!unzip train_100Hz.zip","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VkgtYi_hFKSv","outputId":"7e9d8fcd-ba7a-496d-d24b-7768b21a4c67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"sed: -e expression #1, char 38: Invalid range end\n\n--2023-05-22 02:18:39--  https://docs.google.com/uc?export=download&confirm=&id=1OfvUx1jCva-TR97_GsguAQpd_0rP4HOx\n\nResolving docs.google.com (docs.google.com)... 74.125.141.139, 74.125.141.113, 74.125.141.101, ...\n\nConnecting to docs.google.com (docs.google.com)|74.125.141.139|:443... connected.\n\nHTTP request sent, awaiting response... 303 See Other\n\nLocation: https://doc-0k-64-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tcijk7ubv1ilrlsp75edmrab2otqkjtt/1684721850000/02832111001387922702/*/1OfvUx1jCva-TR97_GsguAQpd_0rP4HOx?e=download&uuid=e93c302c-d343-4648-bc61-b1a1ff41a868 [following]\n\nWarning: wildcards not supported in HTTP.\n\n--2023-05-22 02:18:42--  https://doc-0k-64-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tcijk7ubv1ilrlsp75edmrab2otqkjtt/1684721850000/02832111001387922702/*/1OfvUx1jCva-TR97_GsguAQpd_0rP4HOx?e=download&uuid=e93c302c-d343-4648-bc61-b1a1ff41a868\n\nResolving doc-0k-64-docs.googleusercontent.com (doc-0k-64-docs.googleusercontent.com)... 173.194.210.132, 2607:f8b0:400c:c0f::84\n\nConnecting to doc-0k-64-docs.googleusercontent.com (doc-0k-64-docs.googleusercontent.com)|173.194.210.132|:443... connected.\n\nHTTP request sent, awaiting response... 200 OK\n\nLength: 1490301 (1.4M) [application/zip]\n\nSaving to: ‘train_100Hz.zip’\n\n\n\ntrain_100Hz.zip     100%[===================>]   1.42M  --.-KB/s    in 0.05s   \n\n\n\n2023-05-22 02:18:42 (26.0 MB/s) - ‘train_100Hz.zip’ saved [1490301/1490301]\n\n\n\nArchive:  train_100Hz.zip\n\n   creating: train_100Hz/\n\n  inflating: __MACOSX/._train_100Hz  \n\n  inflating: train_100Hz/1_1683000577.552974_label.csv  \n\n  inflating: train_100Hz/1_1679367565.304249_label.csv  \n\n  inflating: train_100Hz/2_1679635953.098527_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._2_1679635953.098527_LabelData.csv  \n\n  inflating: train_100Hz/1_1679654023.926344_label.csv  \n\n  inflating: train_100Hz/1_1679329841.565424_label.csv  \n\n  inflating: __MACOSX/train_100Hz/._1_1679329841.565424_label.csv  \n\n  inflating: train_100Hz/1_1679332115.60967_label.csv  \n\n  inflating: __MACOSX/train_100Hz/._1_1679332115.60967_label.csv  \n\n  inflating: train_100Hz/18_1681286665.879868_RawData_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._18_1681286665.879868_RawData_LabelData.csv  \n\n  inflating: train_100Hz/.DS_Store   \n\n  inflating: __MACOSX/train_100Hz/._.DS_Store  \n\n  inflating: train_100Hz/18_1681285320.568341_RawData_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._18_1681285320.568341_RawData_LabelData.csv  \n\n  inflating: train_100Hz/18_1682587985.667902_RawData_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._18_1682587985.667902_RawData_LabelData.csv  \n\n  inflating: train_100Hz/3_1681704318.216448_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._3_1681704318.216448_LabelData.csv  \n\n  inflating: train_100Hz/21_1682491580.904765_RawData_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._21_1682491580.904765_RawData_LabelData.csv  \n\n  inflating: train_100Hz/18_1681283899.034908_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._18_1681283899.034908_LabelData.csv  \n\n  inflating: train_100Hz/3_1683012826.916711_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._3_1683012826.916711_LabelData.csv  \n\n  inflating: train_100Hz/1_1681225628.325423_label.csv  \n\n  inflating: train_100Hz/1_1681228619.33164_label.csv  \n\n  inflating: train_100Hz/1_1679717366.680843_label.csv  \n\n  inflating: train_100Hz/18_1681286741.501714_RawData_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._18_1681286741.501714_RawData_LabelData.csv  \n\n  inflating: train_100Hz/1_1681446182.466649_label.csv  \n\n  inflating: train_100Hz/1_1679653662.686183_label.csv  \n\n  inflating: train_100Hz/1_1681229118.636814_label.csv  \n\n  inflating: train_100Hz/1_1679654662.494326_label.csv  \n\n  inflating: train_100Hz/1_1683020280.405568_label.csv  \n\n  inflating: train_100Hz/1_1679771170.914631_label.csv  \n\n  inflating: train_100Hz/1_1679395072.812298_label.csv  \n\n  inflating: __MACOSX/train_100Hz/._1_1679395072.812298_label.csv  \n\n  inflating: train_100Hz/1_1681449591.782904_label.csv  \n\n  inflating: train_100Hz/1_1681447121.996839_label.csv  \n\n  inflating: train_100Hz/18_1681283666.901816_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._18_1681283666.901816_LabelData.csv  \n\n  inflating: train_100Hz/1_1681228013.69366_label.csv  \n\n  inflating: train_100Hz/1_1679397640.947452_label.csv  \n\n  inflating: train_100Hz/1_1681222875.916781_label.csv  \n\n  inflating: train_100Hz/18_1682404281.14869_RawData_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._18_1682404281.14869_RawData_LabelData.csv  \n\n  inflating: train_100Hz/3_1681723164.198223_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._3_1681723164.198223_LabelData.csv  \n\n  inflating: train_100Hz/3_1683015816.045626_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._3_1683015816.045626_LabelData.csv  \n\n  inflating: train_100Hz/1_1679716646.511409_label.csv  \n\n  inflating: train_100Hz/1_1679389849.351148_label.csv  \n\n  inflating: __MACOSX/train_100Hz/._1_1679389849.351148_label.csv  \n\n  inflating: train_100Hz/1_1683000769.194971_label.csv  \n\n  inflating: train_100Hz/1_1679331101.554153_label.csv  \n\n  inflating: __MACOSX/train_100Hz/._1_1679331101.554153_label.csv  \n\n  inflating: train_100Hz/1_1681221446.0114_label.csv  \n\n  inflating: train_100Hz/1_1679982945.155643_label.csv  \n\n  inflating: train_100Hz/1_1679728427.712713_label.csv  \n\n  inflating: train_100Hz/18_1681284177.285987_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._18_1681284177.285987_LabelData.csv  \n\n  inflating: train_100Hz/3_1681704702.329676_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._3_1681704702.329676_LabelData.csv  \n\n  inflating: train_100Hz/22_1682491900.592947_RawData_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._22_1682491900.592947_RawData_LabelData.csv  \n\n  inflating: train_100Hz/1_1679332756.399197_label.csv  \n\n  inflating: __MACOSX/train_100Hz/._1_1679332756.399197_label.csv  \n\n  inflating: train_100Hz/25_1683015178.181543_RawData_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._25_1683015178.181543_RawData_LabelData.csv  \n\n  inflating: train_100Hz/18_1681283439.285306_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._18_1681283439.285306_LabelData.csv  \n\n  inflating: train_100Hz/18_1681285055.514794_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._18_1681285055.514794_LabelData.csv  \n\n  inflating: train_100Hz/1_1681448889.285996_label.csv  \n\n  inflating: train_100Hz/1_1681228473.475849_label.csv  \n\n  inflating: train_100Hz/1_1683001371.622622_label.csv  \n\n  inflating: train_100Hz/1_1681226000.595372_label.csv  \n\n  inflating: train_100Hz/1_1681226580.554961_label.csv  \n\n  inflating: train_100Hz/18_1681284877.125337_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._18_1681284877.125337_LabelData.csv  \n\n  inflating: train_100Hz/3_1681704102.97781_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._3_1681704102.97781_LabelData.csv  \n\n  inflating: train_100Hz/1_1682497923.869615_label.csv  \n\n  inflating: train_100Hz/18_1681285528.529668_RawData_LabelData.csv  \n\n  inflating: __MACOSX/train_100Hz/._18_1681285528.529668_RawData_LabelData.csv  \n"}]},{"cell_type":"code","source":"def delete_ds_store(folder_path):\n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            if file == \".DS_Store\":\n                file_path = os.path.join(root, file)\n                os.remove(file_path)\n                print(\"Deleted:\", file_path)\n\ndelete_ds_store(\"./train_100Hz/\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DsbJMImbFWas","outputId":"6ae5e2ef-d624-4213-ada3-672d13641a8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Deleted: ./train_100Hz/.DS_Store\n"}]},{"cell_type":"code","source":"#sheet = pd.read_csv('/content/sample_1.csv')\n#data_label = sheet.iloc[:,:].values\n#print(data_label.shape[0])\n\ndef cut_data(data_label, size, overlap):\n    data = np.zeros((1,size))\n    label = np.zeros((1,size))\n    end = size\n    start = 0 \n    length = data_label.shape[0]\n    while end < length:\n      data = np.append(data, data_label[start:end,0].reshape((1,size)), axis =0)\n      label = np.append(label, data_label[start:end,1].reshape((1,size)), axis =0)\n      start = end - overlap \n      end = start + size\n    else:\n      data = np.append(data, data_label[length - size:length,0].reshape((1,size)), axis =0)\n      label = np.append(label,data_label[length - size:length,1].reshape((1,size)), axis =0)\n      \n    return data[1:], label[1:] ","metadata":{"id":"FbT5tq2bFaNY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_signal_ratio(arr):\n    count_ones = np.count_nonzero(arr == 1)\n    ratio_ones = count_ones / arr.size\n    return ratio_ones","metadata":{"id":"RSWadSqwNpAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder_path = './train_100Hz/'\nfiles = os.listdir(folder_path)\nsize = 700\ntrain_x = np.zeros((1,size))\ntrain_y = np.zeros((1,size))\n\ntest_x = np.zeros((1,size))\ntest_y = np.zeros((1,size))\n\nfor i, file in enumerate(files):  \n    print(file)\n    sample = pd.read_csv(os.path.join(folder_path, file))\n    data_label = sample.iloc[:,:].values\n    #print(calculate_signal_ratio(data_label[:,1]))\n    #print(data_label.shape)\n    signal_ratio = calculate_signal_ratio(data_label[:,1])\n    test_list = ['1_1679771170.914631_label.csv', '1_1679332756.399197_label.csv', '18_1682404281.14869_RawData_LabelData.csv', \\\n                 '22_1682491900.592947_RawData_LabelData.csv']\n    if file in test_list:\n        t_data_add, t_label_add = cut_data(data_label, size, 50)\n        test_x = np.concatenate((test_x, t_data_add), axis =0)\n        test_y = np.concatenate((test_y, t_label_add), axis =0)\n    else:\n        if signal_ratio > 0.4:\n            data_label = np.concatenate((data_label, data_label, data_label, data_label, data_label, data_label), axis=0)\n            #print(calculate_signal_ratio(data_label[:,1]))\n        if data_label.shape[0] > size: \n            data_add, label_add = cut_data(data_label, size, 50)\n            train_x = np.concatenate((train_x, data_add), axis =0)\n            train_y = np.concatenate((train_y, label_add), axis =0)\n            if calculate_signal_ratio(label_add)>0.7:\n                 train_x = np.concatenate((train_x, data_add), axis =0)\n                 train_y = np.concatenate((train_y, label_add), axis =0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x = train_x[1:]\ntrain_x = train_x.astype(np.float32)\ntrain_y = train_y[1:] \ntrain_y = train_y.astype(np.int64)\n\ntest_x = test_x[1:]\ntest_x = test_x.astype(np.float32)\ntest_y = test_y[1:] \ntest_y = test_y.astype(np.int64)\n\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LyD2z6fqFj6j","outputId":"734d74b8-97ae-4452-b70f-fe26a9725837"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"(6761, 440)\n\n(6761, 440)\n\n(414, 440)\n\n(414, 440)\n"}]},{"cell_type":"code","source":"print(f\"Ratio of ones: {calculate_signal_ratio(train_y)}\")\nprint(f\"Ratio of ones: {calculate_signal_ratio(test_y)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aI3Hxsj_BaU4","outputId":"4c21053f-6673-4a4e-facc-7d0ae8ad8bee"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"Ratio of ones: 0.4885005580132041\n\nRatio of ones: 0.5172101449275363\n"}]},{"cell_type":"code","source":"import pywt\n\n# 定義連續小波轉換（CWT）函數\ndef cwt_transform(data):\n    wavelet = 'morl'  # 使用Morlet小波\n    scales = np.arange(1, 129)  # 定義尺度範圍\n\n    cwt_coeffs = []\n    for sample in data:\n        coeffs, _ = pywt.cwt(sample, scales, wavelet)\n        cwt_coeffs.append(coeffs)\n\n    cwt_coeffs = np.array(cwt_coeffs)\n    return cwt_coeffs\n\n\n# 執行CWT轉換\ntrain_x_cwt = cwt_transform(train_x)\ntest_x_cwt = cwt_transform(test_x)\n\n# 確保train_x_cwt的維度與train_x相同\nprint(train_x_cwt.shape)  # (number, 128, 700)\nprint(test_x_cwt.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def average_pooling(cwt_result, scale_factor):\n    #(number, 32, 4, 700)\n    return np.mean(cwt_result.reshape(cwt_result.shape[0], -1, scale_factor, cwt_result.shape[2]), axis=2)\n\n\nscale_factor = 4\ntrain_x_cwt = average_pooling(train_x_cwt, scale_factor)\ntest_x_cwt = average_pooling(test_x_cwt, scale_factor)\n\nprint(train_x_cwt.shape)  # (number, 32, 700)\nprint(test_x_cwt.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the amplitude scaling range\nscale_range = (0.5, 1.5)\n\n# Define the amplitude scaling transformation\ndata_transform = transforms.Lambda(lambda x: x * random.uniform(scale_range[0], scale_range[1]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EPG_Dataset(Dataset):\n    def __init__(self, x, x_cwt, y, transform=None):\n        self.x = x\n        self.x_cwt = x_cwt\n        self.y = y\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, index):\n        X = self.x[index]\n        CWT = self.x_cwt[index]\n        Y = self.y[index]\n\n        if self.transform is not None:\n            X = self.transform(X)\n\n        return X, CWT, Y","metadata":{"id":"MepcaOfv8gvS"},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"valid_num = int(train_x.shape[0]*0.2)\ntrain_num = train_x.shape[0] - valid_num\n\nbatch_size = 500\nwhole_set = EPG_Dataset(train_x, train_x_cwt, train_y)\ntest_set = EPG_Dataset(test_x, test_x_cwt, test_y)\ntrain_set, valid_set = torch.utils.data.random_split(whole_set,[train_num,valid_num])\n\nwhole_loader = DataLoader(whole_set, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)\n\nprint('Size of testing data:', len(test_set))\nprint('Size of training data:', len(train_set))\nprint('Size of testing data:', len(valid_set))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-GId3fawFrGn","outputId":"9d42dae9-b899-4dbe-ff9c-0713027268f6"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":"Size of training data: 5409\n\nSize of testing data: 1352\n"}]},{"cell_type":"code","source":"class EPGTransformerCNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(EPGTransformerCNN, self).__init__()\n        #self.cnn = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n        self.cnn1d = nn.Sequential(\n            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1), #[16,size/2]\n            nn.MaxPool1d(2, 2),\n            nn.BatchNorm1d(16),\n            nn.ReLU(),\n        \n            nn.Conv1d(16, 32, 3, 1, 1),  #[32, size/4]\n            nn.MaxPool1d(2, 2),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),   \n        )     \n        self.cnn2d = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1), #[8,16,size/2]\n            nn.MaxPool2d(2, 2),\n            nn.BatchNorm2d(8),\n            nn.ReLU(),\n        \n            nn.Conv2d(8, 16, 3, 1, 1),  #[16,8,size/4]\n            nn.MaxPool2d(2, 2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(), \n        )     \n        self.fc1 = nn.Sequential(\n            nn.Linear(8*size, 1000),\n            nn.Dropout(0.15),\n            nn.ReLU(),\n            nn.Linear(1000, 150),\n            nn.Dropout(0.15),\n        )\n        self.fc2 = nn.Sequential(\n            nn.Linear(4*8*size, 5000),\n            nn.Dropout(0.15),\n            nn.ReLU(),\n            nn.Linear(5000, 1000),\n            nn.Dropout(0.15),\n            nn.ReLU(),\n            nn.Linear(1000, 150),\n            nn.Dropout(0.15),\n        )\n        self.transformer = nn.Transformer(d_model=hidden_size, nhead=4, num_encoder_layers=2)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x, x_cwt):\n        x = x.unsqueeze(1)  # (batch_size, 1, size)\n        out_x = self.cnn1d(x)\n        out_x = out_x.view(out_x.size()[0], -1)  # (batch_size, 16*size)\n        out_x = self.fc1(out_x) \n        \n        x_cwt = x_cwt.unsqueeze(1)\n        out_cwt = self.cnn2d(x_cwt)\n        out_cwt = out_cwt.view(out_cwt.size()[0], -1) \n        out_cwt = self.fc2(out_cwt) \n        \n        out = torch.cat((out_x, out_cwt), dim=1)\n        \n        out = self.transformer(out, out)\n        out = self.out(out)\n        out = self.sigmoid(out)\n        return out","metadata":{"id":"Xcnh0bokWe5-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def testing(model):\n    model.eval()\n    test_acc, test_loss = 0.0,0\n    with torch.no_grad():\n        for i, data in enumerate(test_loader):\n            test_pred = model(data[0].cuda(), data[1].cuda())\n            batch_loss = criterion(test_pred, data[2].cuda().float())\n            test_acc += np.sum(torch.round(test_pred.cpu()).data.numpy() == data[2].numpy())\n            #test_loss += batch_loss.item()\n            #print(torch.round(test_pred.cpu()).data.numpy())\n            #print(data[1])\n            #print(\"Test Acc:{}, Test Loss:{}\".format(test_acc/(test_set.__len__()*size), test_loss/(test_set.__len__()*size)))\n            \n    return test_acc/(test_set.__len__()*size)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6eij6NbC6YKi","outputId":"7903c3ae-4968-435e-c691-5d928b9c0587"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"[[0. 0. 0. ... 0. 0. 0.]\n\n [0. 0. 0. ... 0. 0. 0.]\n\n [1. 1. 1. ... 1. 1. 1.]\n\n ...\n\n [0. 0. 0. ... 0. 0. 0.]\n\n [1. 1. 1. ... 1. 1. 1.]\n\n [0. 0. 0. ... 0. 0. 0.]]\n\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n\n        [1, 1, 1,  ..., 1, 1, 1],\n\n        [0, 0, 0,  ..., 1, 1, 1],\n\n        ...,\n\n        [0, 0, 0,  ..., 0, 0, 0],\n\n        [1, 1, 1,  ..., 1, 1, 1],\n\n        [1, 1, 1,  ..., 1, 1, 1]])\n\n[[0. 0. 0. ... 0. 0. 0.]\n\n [1. 1. 1. ... 1. 1. 1.]\n\n [1. 1. 1. ... 1. 1. 1.]\n\n ...\n\n [0. 0. 0. ... 0. 0. 0.]\n\n [0. 0. 0. ... 0. 0. 0.]\n\n [1. 1. 1. ... 1. 1. 1.]]\n\ntensor([[1, 1, 1,  ..., 1, 1, 1],\n\n        [1, 1, 1,  ..., 1, 1, 1],\n\n        [1, 1, 1,  ..., 1, 1, 1],\n\n        ...,\n\n        [0, 0, 0,  ..., 0, 0, 0],\n\n        [0, 0, 0,  ..., 0, 0, 0],\n\n        [1, 1, 1,  ..., 1, 1, 1]])\n\n[[1. 1. 1. ... 1. 1. 1.]\n\n [1. 1. 1. ... 1. 1. 1.]\n\n [0. 0. 0. ... 0. 0. 0.]\n\n ...\n\n [1. 1. 1. ... 1. 1. 1.]\n\n [0. 0. 0. ... 0. 0. 0.]\n\n [0. 0. 0. ... 0. 0. 0.]]\n\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n\n        [1, 1, 1,  ..., 1, 1, 1],\n\n        [0, 0, 0,  ..., 0, 0, 0],\n\n        ...,\n\n        [1, 1, 1,  ..., 1, 1, 1],\n\n        [0, 0, 0,  ..., 0, 0, 0],\n\n        [0, 0, 0,  ..., 0, 0, 0]])\n\nTest Acc:0.8426444488266615, Test Loss:2.3960417300249193e-06\n"}]},{"cell_type":"code","source":"input_size = size\nhidden_size = 300\noutput_size = size\n\nmodel = EPGTransformerCNN(input_size, hidden_size, output_size).cuda()\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.000005)\nnum_epoch = 85\ntrain_epochs = []\ntest_epochs = []\n\nfor epoch in range(num_epoch):\n    epoch_start_time = time.time()\n    train_acc = 0.0\n    train_loss = 0.0\n\n    model.train()\n    for i, data in enumerate(whole_loader):\n        optimizer.zero_grad()\n        train_pred = model(data[0].cuda(), data[1].cuda())\n        batch_loss = criterion(train_pred, data[2].cuda().float())\n        batch_loss.backward()\n        optimizer.step()\n        train_acc += np.sum(torch.round(train_pred.cpu()).data.numpy().reshape(-1,size) == data[2].numpy())\n        train_loss += batch_loss.item()\n        #print(torch.round(train_pred.cpu()).data.numpy().reshape(-1,size))\n        #print(data[1].numpy())\n        \n    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n      train_acc/(whole_set.__len__()*size), train_loss/(whole_set.__len__()*size)))\n    \n    if (epoch+1) % 5 == 0: \n        train_epochs.append(train_acc/(whole_set.__len__()*size))\n        a = testing(model)\n        print(a)\n        test_epochs.append(a)\n\nprint(train_epochs)\nprint(test_epochs)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1TYiSiZBXg9m","outputId":"3b66a650-5d27-4868-a0ea-6f887ba229d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"[001/025] 0.73 sec(s) Train Acc: 0.498615 Loss: 0.000004\n\n[002/025] 0.62 sec(s) Train Acc: 0.498994 Loss: 0.000003\n\n[003/025] 0.64 sec(s) Train Acc: 0.499418 Loss: 0.000003\n\n[004/025] 0.62 sec(s) Train Acc: 0.500186 Loss: 0.000003\n\n[005/025] 0.62 sec(s) Train Acc: 0.500333 Loss: 0.000003\n\n[006/025] 0.63 sec(s) Train Acc: 0.501426 Loss: 0.000003\n\n[007/025] 0.63 sec(s) Train Acc: 0.500986 Loss: 0.000003\n\n[008/025] 0.62 sec(s) Train Acc: 0.501150 Loss: 0.000003\n\n[009/025] 0.62 sec(s) Train Acc: 0.502365 Loss: 0.000003\n\n[010/025] 0.62 sec(s) Train Acc: 0.507947 Loss: 0.000003\n\n[011/025] 0.62 sec(s) Train Acc: 0.524899 Loss: 0.000003\n\n[012/025] 0.64 sec(s) Train Acc: 0.567226 Loss: 0.000003\n\n[013/025] 0.72 sec(s) Train Acc: 0.615871 Loss: 0.000003\n\n[014/025] 0.74 sec(s) Train Acc: 0.664300 Loss: 0.000003\n\n[015/025] 0.73 sec(s) Train Acc: 0.720675 Loss: 0.000003\n\n[016/025] 0.63 sec(s) Train Acc: 0.763101 Loss: 0.000003\n\n[017/025] 0.63 sec(s) Train Acc: 0.798421 Loss: 0.000003\n\n[018/025] 0.64 sec(s) Train Acc: 0.820445 Loss: 0.000002\n\n[019/025] 0.64 sec(s) Train Acc: 0.841702 Loss: 0.000002\n\n[020/025] 0.63 sec(s) Train Acc: 0.857955 Loss: 0.000002\n\n[021/025] 0.63 sec(s) Train Acc: 0.870992 Loss: 0.000002\n\n[022/025] 0.63 sec(s) Train Acc: 0.867437 Loss: 0.000002\n\n[023/025] 0.65 sec(s) Train Acc: 0.882314 Loss: 0.000002\n\n[024/025] 0.63 sec(s) Train Acc: 0.888155 Loss: 0.000002\n\n[025/025] 0.63 sec(s) Train Acc: 0.896600 Loss: 0.000002\n"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), './model_ep34.pt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ntest_acc, test_loss = 0.0,0\n\nwith torch.no_grad():\n        for i, data in enumerate(test_loader):\n            test_pred = model(data[0].cuda(), data[1].cuda())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef draw_pic(index):\n    a = data[0].cpu().numpy()[index, :]\n    b = torch.round(test_pred.cpu()).data.numpy()[index, :]\n    x = np.arange(size)\n\n    fig, ax = plt.subplots()\n\n    # 绘制折线图\n    ax.plot(x, a, color='gray', label='data')\n\n    # 绘制 a 中 b 为 1 的点，颜色为红色，大小为 5\n    ax.scatter(x[b == 1], a[b == 1], c='red', label='signal', s=5)\n\n    # 绘制 a 中 b 为 0 的点，颜色为蓝色，大小为 5\n    ax.scatter(x[b == 0], a[b == 0], c='blue', label='noise', s=5)\n\n    ax.legend()\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    plt.show()\n    filename = f'plot_prediction_{index}.png'\n    fig.savefig(filename)\n\n\nfor i in range(20):\n    draw_pic(i)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lr=0.00001\n#num_epoch = 15, [015/015] 0.55 sec(s) Train Acc: 0.614801 Loss: 0.000003 Test Acc:0.6178011643528885\n#num_epoch = 17, [017/017] 0.53 sec(s) Train Acc: 0.592808 Loss: 0.000003 Test Acc:0.5648612750885478\n#num_epoch = 20, [020/020] 0.54 sec(s) Train Acc: 0.800360 Loss: 0.000002 Test Acc:0.4484848484848485\n#num_epoch = 20, [020/020] 0.56 sec(s) Train Acc: 0.769532 Loss: 0.000003 Test Acc:0.8133448184468592  ***\n#num_epoch = 20 [020/020] 0.58 sec(s) Train Acc: 0.677996 Loss: 0.000003 Test Acc:0.48254545454545456\n#num_epoch = 25, [025/025] 0.57 sec(s) Train Acc: 0.818442 Loss: 0.000002 Test Acc:0.29112383679312814\n\n#lr=0.000005\n#num_epoch = 25, [025/025] 0.59 sec(s) Train Acc: 0.696756 Loss: 0.000003 Test Acc:0.5743006993006993\n#num_epoch = 30, [030/030] 0.54 sec(s) Train Acc: 0.655785 Loss: 0.000003 Test Acc:0.522711038961039\n#num_epoch = 33  [033/033] 0.55 sec(s) Train Acc: 0.763456 Loss: 0.000003 Test Acc:0.7260200429491768\n#num_epoch = 34  [034/034] 0.65 sec(s) Train Acc: 0.802218 Loss: 0.000002 Test Acc:0.7817303469477382\n#num_epoch = 34  [034/034] 0.55 sec(s) Train Acc: 0.795996 Loss: 0.000003 Test Acc:0.8938148609381487 ***\n#num_epoch = 34  [034/034] 0.54 sec(s) Train Acc: 0.801585 Loss: 0.000002 Test Acc:0.7659365393061045\n#num_epoch = 35, [035/035] 0.56 sec(s) Train Acc: 0.804331 Loss: 0.000002 Test Acc:0.4920320855614973\n\n#[0.5928 0.6148 0.6557 0.6780 0.6967 0.7634 0.7695 0.7960 0.8004 0.8022 0.8043 0.81844]\n#[0.5649 0.6178 0.5227 0.4825 0.5743 0.7260 0.8133 0.8938 0.4485 0.7817 0.4920 0.2911] \n\n#(5733, 440)\n#(198, 440)\n\n#Transform/Test num =198\n#lr=0.000005\n#[040/040] 0.48 sec(s) Train Acc: 0.825898 Loss: 0.000002 Test Acc:0.5975665748393021\n#[060/060] 0.48 sec(s) Train Acc: 0.827733 Loss: 0.000002 Test Acc:0.6296946740128558\n#[080/080] 0.46 sec(s) Train Acc: 0.857112 Loss: 0.000002 Test Acc:0.6735078053259872\n#Transform 0.7-1.3/Test num =198\n#lr=0.000005\n#[100/100] 0.47 sec(s) Train Acc: 0.854072 Loss: 0.000002 Test Acc:0.6317837465564738\n#[120/120] 0.48 sec(s) Train Acc: 0.864163 Loss: 0.000002 Test Acc:0.659159779614325\n \n#larger model + transform(0.7,1.3)\n#lr=0.000005\n#[080/080] 0.49 sec(s) Train Acc: 0.884652 Loss: 0.000002 Test Acc:0.654912764003673\n#[100/100] 0.48 sec(s) Train Acc: 0.921284 Loss: 0.000001 Test Acc:0.690805785123967\n#[120/120] 0.50 sec(s) Train Acc: 0.941723 Loss: 0.000001 Test Acc:0.7061179981634527\n#[140/140] 0.49 sec(s) Train Acc: 0.943499 Loss: 0.000001 Test Acc:0.7431473829201102 + 以下都加dropout(0.1)\n#[160/160] 0.48 sec(s) Train Acc: 0.951038 Loss: 0.000001 Test Acc:0.6757575757575758\n#[180/180] 0.49 sec(s) Train Acc: 0.960227 Loss: 0.000001 Test Acc:0.7108356290174472\n#+dropout(0.15)\n#[140/140] 0.49 sec(s) Train Acc: 0.923211 Loss: 0.000001 Test Acc:0.7408057851239669\n#[140/140] 0.48 sec(s) Train Acc: 0.919488 Loss: 0.000001 Test Acc:0.6913452708907254\n#+dropout(0.15)+ transform(0.5,1.5)\n#[150/150] 0.49 sec(s) Train Acc: 0.914206 Loss: 0.000001 Test Acc:0.6730716253443526\n#+dropout(0.2)\n#[140/140] 0.49 sec(s) Train Acc: 0.944264 Loss: 0.000001 Test Acc:0.7093204775022957","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#+CWT & size= 700\n#epoch = 150\n#Test Acc: [0.49467857 0.49645238 0.50172619 0.55271429 0.64540476 0.70619048 \n#0.72659524 0.75847619 0.73310714 0.72279762 0.71645238 0.75432143\n#0.7649881  0.75603571 0.76640476 0.76761905 0.76778571 0.77469048\n#0.75641667 0.75457143 0.79108333 0.75794048 0.75528571 0.71125\n#0.75777381 0.69659524 0.70514286 0.70814286 0.75230952 0.7524881]","metadata":{},"execution_count":null,"outputs":[]}]}